---
title: Greater Early Disambiguating Information for Less-Probable Words
subtitle: The Lexicon Is Shaped by Incremental Processing
layout: post
date: 2021-02-01
keywords: language efficiency, Zipf's law of abbreviation, incremental processing, language evolution, information theory
published: true
categories: ["project"]
---

Here are some words. And this is some math:

{% katexmm %}
$$h(\text{seg}_n) = -\log_2{\frac{\text{count}(\text{seg}_1\dots\text{seg}_{n})}{\text{count}(\text{seg}_1\dots\text{seg}_{n-1})}}$$

$$ e = mc^2. \tag{1} $$
{% endkatexmm %}

{% cite bishop2006pattern %}

<div class='figure'>
    <img src="/assets/images/transparent.png"
         style="width: 60%; display: block; margin: 0 auto;"/>
    <div class='caption'>
        <span class='caption-label'>Figure 1.</span> Diagram of rejection sampling. The 
        density $q(\mathbf{z})$ must be always greater than $p(\mathbf{x})$. A new sample 
        is rejected if it falls in the gray region and accepted otherwise. These accepted 
        samples are distributed according to $p(\mathbf{x})$. This is achieved by sampling 
        $z_i$ from $q(\mathbf{z})$, and then sampling uniformly from $[0, k q(z_i)]$. 
        Samples under the curve $p(z_i)$ are accepted.
    </div>
</div>

```ruby
def print_hi(name)
  puts "Hi, #{name}"
end
print_hi('Tom')
#=> prints 'Hi, Tom' to STDOUT.
```

Cool!